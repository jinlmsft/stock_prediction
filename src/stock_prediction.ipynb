{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock prediction from earnings call transcript\n",
    "Modify 2nd cell with company name and url of latest earnings call from seekingalpha.com, then run all cells sequentially.You might also need to add latest earnings surprise to the XX_eps.csv file under data/ as well if it is not updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ...Loading Master Dictionary 85000\n",
      "Master Dictionary loaded from file: \n",
      "  LoughranMcDonald_MasterDictionary_2014.csv\n",
      "  85,131 words loaded in master_dictionary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas_datareader as pdr\n",
    "import Load_MasterDictionary as LM\n",
    "import pickle\n",
    "import os\n",
    "MASTER_DICTIONARY_FILE = r'LoughranMcDonald_MasterDictionary_2014.csv'\n",
    "lm_dictionary = LM.load_masterdictionary(MASTER_DICTIONARY_FILE, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://seekingalpha.com/article/4237346-amazon-com-inc-amzn-q4-2018-results-earnings-call-transcript\"\n",
    "company_name = 'Amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'\n",
    "\n",
    "def download_url(url):\n",
    "    import urllib.request\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": user_agent,\n",
    "        \"referrer\": 'https://google.com',\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\"\n",
    "    }\n",
    "    req = urllib.request.Request(\n",
    "        url, \n",
    "        data=None, \n",
    "        headers=headers\n",
    "    )\n",
    "\n",
    "    f = urllib.request.urlopen(req)\n",
    "\n",
    "    return f.read().decode('utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_earnings_call(url):\n",
    "    html = download_url(url)\n",
    "    soup_obj = BeautifulSoup(html, 'html.parser')\n",
    "    #get the title\n",
    "    title_obj = soup_obj.find_all(\"h1\")\n",
    "    title = title_obj[0].get_text()\n",
    "    #get company name\n",
    "    company_name = title.split()[0]\n",
    "    #get time stamp string\n",
    "    time_str = soup_obj.find(\"div\",{\"class\":\"a-info clearfix\"}).find(\"time\")['content']\n",
    "    #get full text\n",
    "    paras = soup_obj.find_all(\"p\")\n",
    "    full_text = \"\\n\".join([p.text for p in paras])\n",
    "    return time_str,company_name,title,full_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split one transcript into intro and Q&A\n",
    "#compute the tone for both parts \n",
    "def parse_one_call_transcript(doc):\n",
    "    doc = doc.upper()\n",
    "    div = re.findall('QUESTION.{0,2}AND.{0,2}ANSWER.{0,10}\\n|QUESTION.{0,2}&.{0,2}ANSWER.{0,10}\\n',doc)\n",
    "    if len(div) == 0:\n",
    "        div = re.findall('QUESTION.*AND.ANSWER.{0,10}OPERATOR',doc)\n",
    "    if len(div) == 0:\n",
    "        div = re.findall('Q&A.{0,8}',doc)\n",
    "    \n",
    "    sections = doc.split(div[0])\n",
    "    intro = sections[0]\n",
    "    qna = sections[1]\n",
    "    #compute linguistic characteristics using L and McDonauld dictionary \n",
    "    odata_intro = feature_extraction(intro)\n",
    "    odata_qna = feature_extraction(qna)\n",
    "    #tone = positive sentiment - negative sentiment\n",
    "    tone_intro = odata_intro[3] - odata_intro[4]\n",
    "    tone_qna = odata_qna[3] - odata_qna[4]\n",
    "    #compute abnormal tone which is difference btw introduction tone and q&a tone\n",
    "    tone_ab = tone_intro - tone_qna\n",
    "    \n",
    "    return tone_intro,tone_qna, tone_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract featurs from doc using Loughran and McDonald Dictionary\n",
    "def feature_extraction(doc):\n",
    "    \n",
    "    vdictionary = {}\n",
    "    _odata = [0] * 17\n",
    "    total_syllables = 0\n",
    "    word_length = 0\n",
    "    \n",
    "    doc = doc.upper()\n",
    "    tokens = re.findall('\\w+', doc)  # Note that \\w+ splits hyphenated words\n",
    "    for token in tokens:\n",
    "        if not token.isdigit() and len(token) > 1 and token in lm_dictionary:\n",
    "            _odata[2] += 1  # word count\n",
    "            word_length += len(token)\n",
    "            if token not in vdictionary:\n",
    "                vdictionary[token] = 1\n",
    "            if lm_dictionary[token].positive: _odata[3] += 1\n",
    "            if lm_dictionary[token].negative: _odata[4] += 1\n",
    "            if lm_dictionary[token].uncertainty: _odata[5] += 1\n",
    "            if lm_dictionary[token].litigious: _odata[6] += 1\n",
    "            if lm_dictionary[token].weak_modal: _odata[7] += 1\n",
    "            if lm_dictionary[token].moderate_modal: _odata[8] += 1\n",
    "            if lm_dictionary[token].strong_modal: _odata[9] += 1\n",
    "            if lm_dictionary[token].constraining: _odata[10] += 1\n",
    "            total_syllables += lm_dictionary[token].syllables\n",
    "\n",
    "    _odata[11] = len(re.findall('[A-Z]', doc))\n",
    "    _odata[12] = len(re.findall('[0-9]', doc))\n",
    "    # drop punctuation within numbers for number count\n",
    "    doc = re.sub('(?!=[0-9])(\\.|,)(?=[0-9])', '', doc)\n",
    "    doc = doc.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    _odata[13] = len(re.findall(r'\\b[-+\\(]?[$€£]?[-+(]?\\d+\\)?\\b', doc))\n",
    "    _odata[14] = total_syllables / _odata[2]\n",
    "    _odata[15] = word_length / _odata[2]\n",
    "    _odata[16] = len(vdictionary) #number of unqiue words\n",
    "    \n",
    "    # Convert counts to %\n",
    "    for i in range(3, 10 + 1):\n",
    "        _odata[i] = (_odata[i] / _odata[2]) * 100\n",
    "    # Vocabulary\n",
    "        \n",
    "    return _odata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute stock price and volatility change\n",
    "#inputs: time stamp of earnings call, company ticker, number of days after and before call\n",
    "#outputs: price and volatility changes\n",
    "def compute_stock_prop(call_time,ticker,time_window):\n",
    "    start_time = call_time - timedelta(days = time_window[0] + 5)#add extra days to account for weekend \n",
    "    end_time = call_time + timedelta(days = time_window[1] + 5) \n",
    "    stock_data = pdr.get_data_yahoo(ticker,start = start_time,end = end_time).reset_index()\n",
    "    if len(stock_data) < 6:\n",
    "        return None\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    \n",
    "    #split into before and after earnings call\n",
    "    stock_before = stock_data[stock_data['Date'] <= call_time]['Adj Close']\n",
    "    stock_after = stock_data[stock_data['Date'] > call_time]['Adj Close']\n",
    "    stock_before = stock_before[-time_window[0]:]\n",
    "    stock_after = stock_after[0:time_window[1]]\n",
    "    \n",
    "    #compute stock price and volatitiy change\n",
    "    price_change  = (stock_after.mean() - stock_before.mean())/stock_before.mean()\n",
    "    vol_before = stock_before.std()*np.sqrt(len(stock_before))\n",
    "    vol_after = stock_after.std()*np.sqrt(len(stock_after))\n",
    "    vol_change = (vol_after - vol_before)/vol_before\n",
    "    \n",
    "    return price_change,vol_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load earnings surprise data\n",
    "def load_eps(company_name):\n",
    "    path = os.getcwd()\n",
    "    path = path[:-3]\n",
    "    path = path + 'data/'\n",
    "    eps_data = pd.read_csv(path + company_name + '_eps.csv',sep = '\\t',names = ['time','eps'])\n",
    "    eps_data['time'] = pd.to_datetime(eps_data['time'])\n",
    "    eps_data['eps'] = eps_data['eps'].str.replace('%','').apply(float)\n",
    "    return eps_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: url of latest earnings call and company name\n",
    "#output: stock price and volatility change \n",
    "def post_call_stock_prediction(url, company_name):\n",
    "    #scrape the earnings call transcript and put it in a dataframe\n",
    "    time_str,company,title,full_text = get_one_earnings_call(url)\n",
    "    latest_call = pd.DataFrame(columns = ['time','company','title','full_text','url'])\n",
    "    latest_call.loc[0] = [time_str,company,title,full_text,url]\n",
    "    #get company ticker\n",
    "    ticker_list = {\"Apple\":\"AAPL\",\"Amazon\":\"AMZN\",\"Twitter\":\"TWTR\",\"Microsoft\":\"MSFT\",\"IBM\":\"IBM\",\n",
    "                  \"Facebook\":\"FB\",\"Ebay\":\"EBAY\",\"Google\":\"GOOG\",\"Oracle\":\"ORCL\",\"Intel\":\"INTC\"}\n",
    "    ticker = ticker_list[company_name]\n",
    "    # compute the time of earnings call\n",
    "    time_earnings = pd.to_datetime(latest_call['time']) - timedelta(days=0.5)\n",
    "    time_earnings = pd.to_datetime(time_earnings.apply(datetime.date))\n",
    "    # extract tone from the transcript\n",
    "    tones = latest_call['full_text'].apply(parse_one_call_transcript)\n",
    "    tone_intro = []\n",
    "    tone_qna = []\n",
    "    tone_ab = []\n",
    "    for tone in tones:\n",
    "        tone_intro.append(tone[0])\n",
    "        tone_qna.append(tone[1])\n",
    "        tone_ab.append(tone[2])\n",
    "    earnings_stock = pd.DataFrame(time_earnings,columns = ['time'])\n",
    "    earnings_stock['company'] = ticker\n",
    "    earnings_stock['tone_intro'] = tone_intro\n",
    "    earnings_stock['tone_qna'] = tone_qna\n",
    "    earnings_stock['tone_ab'] = tone_ab    \n",
    "    \n",
    "    #get the stock data from Yahoo finance and compute the change\n",
    "    #pick up stock change around the call\n",
    "    time_window = [10,10] #ten days before and after call\n",
    "    stock_change = earnings_stock['time'].apply(compute_stock_prop,args = (ticker,time_window))\n",
    "    price_change = []\n",
    "    vola_change = []\n",
    "    for change in stock_change:\n",
    "        price_change.append(change[0])\n",
    "        vola_change.append(change[1])\n",
    "    \n",
    "    earnings_stock['price_change'] = price_change\n",
    "    earnings_stock['vola_change'] = vola_change   \n",
    "\n",
    "    #load earnings surprise data and merge with earnings call\n",
    "    eps_latest = load_eps(company_name)\n",
    "    data_latest = eps_latest.merge(earnings_stock,on = 'time')\n",
    "    print(data_latest)\n",
    "    X_latest = data_latest[['eps','tone_ab']]\n",
    "    y_latest = data_latest[['price_change','vola_change']]\n",
    "    \n",
    "    #load model\n",
    "    p = os.path.join(os.getcwd(), \"stock_prediction_model.best\")\n",
    "    with open(p, 'rb') as f2:\n",
    "        best_model = pickle.load(f2)\n",
    "    \n",
    "    #compute predicted price and volatility change\n",
    "    y_pred_latest = best_model.predict(X_latest)\n",
    "    real_price_change = round(y_latest['price_change'][0]*10000)/100\n",
    "    predicted_price_change = round(y_pred_latest[0,0]*10000)/100\n",
    "    real_vola_change = round(y_latest['vola_change'][0]*10000)/100\n",
    "    predicted_vola_change = round(y_pred_latest[0,1]*10000)/100\n",
    "    print(company_name +' stock price change:' + str(real_price_change) + '%')\n",
    "    print('predicted price change: ' + str(predicted_price_change) + '%')\n",
    "    print(company_name +' stock volatility change:' + str(real_vola_change) + '%')\n",
    "    print('predicted volatility change: ' + str(predicted_vola_change) + '%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time   eps company  tone_intro  tone_qna   tone_ab  price_change  \\\n",
      "0 2019-01-31  8.83    AMZN   -1.550388  1.096391 -2.646779     -0.020434   \n",
      "\n",
      "   vola_change  \n",
      "0     -0.49277  \n",
      "Amazon stock price change:-2.04%\n",
      "predicted price change: -2.07%\n",
      "Amazon stock volatility change:-49.28%\n",
      "predicted volatility change: 21.51%\n"
     ]
    }
   ],
   "source": [
    "post_call_stock_prediction(url, company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
